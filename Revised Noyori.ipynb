{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMohlsRYNga+hJnRZhA10Rw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Overall reaction free energy barrier for revised Noyori (RN) type mechanism  (ΔΔG$^‡$$^R$$^N$)**"],"metadata":{"id":"XiVniMNdS1ir"}},{"cell_type":"markdown","source":["# **Importing modules**"],"metadata":{"id":"TqcGTLd1Vbu8"}},{"cell_type":"code","source":["#IMPORTING LIBRARIES\n","import numpy as np\n","import pandas as pd\n","from joblib import dump\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#IMPORTING DATAPREPROCESSING TOOLS\n","from sklearn.model_selection import train_test_split,RandomizedSearchCV\n","df = pd.read_excel('Rev_train_test.xlsx')\n","y = df['E_Rev']\n","x = df.iloc[:,1:17]\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n","\n","\n","#IMPORTING MODELS\n","from sklearn.linear_model import LinearRegression\n","from sklearn.kernel_ridge import KernelRidge\n","from sklearn.ensemble import GradientBoostingRegressor as GBR\n","from sklearn.ensemble import RandomForestRegressor as RF\n","import xgboost as xgb\n","\n","#IMPORTING CROSS VALIDATION TOOLS\n","from sklearn.metrics import mean_squared_error as mse\n","from sklearn.model_selection import KFold,cross_val_score\n","from math import sqrt\n","from sklearn.utils import shuffle"],"metadata":{"id":"laMOm4VrPovc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Linear regression**"],"metadata":{"id":"QL_20H4UZTIs"}},{"cell_type":"code","source":["reg = LinearRegression()\n","param_grid = {'fit_intercept': ['True', 'False'], 'normalize': ['True', 'False']}\n","rmse = []\n","num = []\n","best_param = []\n","#Running a 'for' loop to check best metrics scores by varying hyperparameters\n","for i in range(500):\n","  lr = RandomizedSearchCV(reg, param_distributions=param_grid,cv=5,random_state=i)\n","  lr.fit(x_train,y_train)\n","  best_param.append(lr.best_params_)\n","  best_randomcv = lr.best_estimator_\n","  y_pred = best_randomcv.predict(x_test)\n","  rmse.append(np.sqrt(mse(y_test,y_pred)))\n","  num.append(i)\n","a = min(rmse)\n","#Running a 'for' loop to print best pararmeters and their corresponding RMSE for each itaration\n","for i,(j,k) in enumerate(zip(best_param,rmse)):\n","  print(i,j,k)\n","#Running a 'for' loop to print best pararmeters and the RMSE\n","for i,(j,k) in enumerate(zip(best_param,rmse)):\n","  if k == a:\n","    print(i,j,k)"],"metadata":{"id":"wITecVoVS_S7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Kernel ridge regression**"],"metadata":{"id":"7yfTsvSCZq8O"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"P20T212l14vU"},"outputs":[],"source":["reg = KernelRidge()\n","param_grid = {\"alpha\": [1e0, 1e-1, 1e-2, 1e-3],\n","              \"kernel\": ['linear', 'rbf','poly', 'sigmoid', 'laplacian'],\n","              \"gamma\": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5],\n","              \"coef0\":[0, 1, 2, 3, 4],\n","              \"degree\":[0, 1, 2, 3, 4, 5]}\n","rmse = []\n","num = []\n","best_param = []\n","#Running a 'for' loop to check best metrics scores by varying hyperparameters\n","for i in range(500):\n","  kr = RandomizedSearchCV(reg, param_distributions=param_grid,cv=5,random_state=i)\n","  kr.fit(x_train,y_train)\n","  best_param.append(kr.best_params_)\n","  best_randomcv = kr.best_estimator_\n","  y_pred = best_randomcv.predict(x_test)\n","  rmse.append(np.sqrt(mse(y_test,y_pred)))\n","  num.append(i)\n","a = min(rmse)\n","#Running a 'for' loop to print best pararmeters and their corresponding RMSE for each itaration\n","for i,(j,k) in enumerate(zip(best_param,rmse)):\n","  print(i,j,k)\n","#Running a 'for' loop to print best pararmeters and the RMSE\n","for i,(j,k) in enumerate(zip(best_param,rmse)):\n","  if k == a:\n","    print(i,j,k)"]},{"cell_type":"markdown","source":["# **Random forest regression**"],"metadata":{"id":"DR-jpgbFaHpS"}},{"cell_type":"code","source":["reg = RF()\n","param_grid = {'bootstrap': [True, False],\n"," 'max_depth': [1,2,3,4,5,6,7,8,9,10,20,30,40,50,None],\n"," 'max_features': ['auto', 'sqrt'],\n"," 'min_samples_leaf': [0,1,2,3,4,5,6,7,8,9,10],\n"," 'min_samples_split': [2,3,4,5,6,7,8,9,10,20,30,40,50,100,150],\n"," 'n_estimators': [10,20,30,40,50,60,70,80,90,100,200,400,600]}\n","rmse = []\n","num = []\n","best_param = []\n","#Running a 'for' loop to check best metrics scores by varying hyperparameters\n","for i in range(500):\n","  kr = RandomizedSearchCV(reg, param_distributions=param_grid,cv=5,random_state=i)\n","  kr.fit(x_train,y_train)\n","  best_param.append(kr.best_params_)\n","  best_randomcv = kr.best_estimator_\n","  y_pred = best_randomcv.predict(x_test)\n","  rmse.append(np.sqrt(mse(y_test,y_pred)))\n","  num.append(i)\n","a = min(rmse)\n","#Running a 'for' loop to print best pararmeters and their corresponding RMSE for each itaration\n","for i,(j,k) in enumerate(zip(best_param,rmse)):\n","  print(i,j,k)\n","#Running a 'for' loop to print best pararmeters and the RMSE\n","for i,(j,k) in enumerate(zip(best_param,rmse)):\n","  if k == a:\n","    print(i,j,k)"],"metadata":{"id":"vuxyBI7SaQHs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Gradient boosting regression**"],"metadata":{"id":"OvtsnyrSasm9"}},{"cell_type":"code","source":["reg = GBR()\n","n_estimators = [100, 500, 900, 1100, 1500]\n","max_depth = [2,3,5,10,15]\n","min_sample_split= [2,5,10,15,20]\n","learning_rate=[0.05,0.1,0.15,0.20]\n","min_sample_leaf=[2,3,5,10,15]\n","xgb_randomgrid = {\n","    'n_estimators': n_estimators,\n","    'max_depth': max_depth,\n","    'learning_rate': learning_rate,\n","    'min_samples_split': min_sample_split,\n","    'min_samples_leaf': min_sample_leaf,\n","    }\n","rmse = []\n","num = []\n","best_param = []\n","#Running a 'for' loop to check best metrics scores by varying hyperparameters\n","for i in range(500):\n","  kr = RandomizedSearchCV(reg, param_distributions=xgb_randomgrid,cv=5,random_state=i)\n","  kr.fit(x_train,y_train)\n","  best_param.append(kr.best_params_)\n","  best_randomcv = kr.best_estimator_\n","  y_pred = best_randomcv.predict(x_test)\n","  rmse.append(np.sqrt(mse(y_test,y_pred)))\n","  num.append(i)\n","a = min(rmse)\n","#Running a 'for' loop to print best pararmeters and their corresponding RMSE for each itaration\n","for i,(j,k) in enumerate(zip(best_param,rmse)):\n","  print(i,j,k)\n","#Running a 'for' loop to print best pararmeters and the RMSE\n","for i,(j,k) in enumerate(zip(best_param,rmse)):\n","  if k == a:\n","    print(i,j,k)"],"metadata":{"id":"8YmwVKh4a0x_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Extreme gradient boosting regression**"],"metadata":{"id":"zW6M9UtObSkN"}},{"cell_type":"code","source":["reg = xgb.XGBRegressor()\n","booster = ['gbtree', 'gblinear']\n","base_score = [0.25,0.5,0.75,1]\n","n_estimators = [100, 500, 900, 1100, 1500]\n","max_depth = [2,3,5,10,15]\n","booster = ['gbtree', 'gblinear']\n","learning_rate=[0.05,0.1,0.15,0.20]\n","min_child_weight=[1,2,3,4]\n","xgb_randomgrid = {\n","    'n_estimators': n_estimators,\n","    'max_depth': max_depth,\n","    'learning_rate': learning_rate,\n","    'min_child_weight': min_child_weight,\n","    'booster': booster,\n","    'base_score': base_score,\n","    }\n","rmse = []\n","num = []\n","best_param = []\n","#Running a 'for' loop to check best metrics scores by varying hyperparameters\n","for i in range(500):\n","  kr = RandomizedSearchCV(reg, param_distributions=xgb_randomgrid,cv=5,random_state=i)\n","  kr.fit(x_train,y_train)\n","  best_param.append(kr.best_params_)\n","  best_randomcv = kr.best_estimator_\n","  y_pred = best_randomcv.predict(x_test)\n","  rmse.append(np.sqrt(mse(y_test,y_pred)))\n","  num.append(i)\n","a = min(rmse)\n","#Running a 'for' loop to print best pararmeters and their corresponding RMSE for each itaration\n","for i,(j,k) in enumerate(zip(best_param,rmse)):\n","  print(i,j,k)\n","#Running a 'for' loop to print best pararmeters and the RMSE\n","for i,(j,k) in enumerate(zip(best_param,rmse)):\n","  if k == a:\n","    print(i,j,k)"],"metadata":{"id":"EZE4DAV-b6fk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Among all the considered models XGBoost found to be the most suitable model**"],"metadata":{"id":"VJIXEZt3Lt34"}},{"cell_type":"markdown","source":["# **Cross validation (CV)**"],"metadata":{"id":"9CZxBVMacM2e"}},{"cell_type":"markdown","source":["# **k-fold Cross-Validation for XGBoost model**"],"metadata":{"id":"ZXrIG2uKfbUX"}},{"cell_type":"code","source":["kf = KFold(n_splits=5,shuffle=True,random_state=10)\n","reg = xgb.XGBRegressor(n_estimators=100, min_child_weight=3, learning_rate=0.05, booster='gbtree', max_depth=5, base_score=0.5)\n","scores = cross_val_score(reg, x, y, cv = kf)\n","from sklearn.utils import shuffle\n","import numpy as np\n","X_shuffle, y_shuffle = shuffle(x, y, random_state=10)\n","from sklearn.model_selection import cross_val_score\n","scores = cross_val_score(reg, X_shuffle, y_shuffle,\n","                         scoring=\"neg_mean_squared_error\",\n","                         cv=5, n_jobs=1)\n","rmse = np.sqrt(-scores)\n","print(\"RMSE values: \", np.round(rmse, 5))\n","print(\"RMSE average: \", np.mean(rmse))"],"metadata":{"id":"1jQ2tOcXeSK0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Permutation feature importance analysis with XGBoost model**"],"metadata":{"id":"IPM7lqDFrQ0-"}},{"cell_type":"code","source":["reg = xgb.XGBRegressor(n_estimators=100, min_child_weight=3, learning_rate=0.05, booster='gbtree', max_depth=5, base_score=0.5)\n","reg.fit(x,y)\n","results = permutation_importance(reg, x, y, scoring= 'neg_mean_squared_error' )\n","importance = results.importances_mean\n","for i,v in enumerate(importance):\n","  print(' Feature: %0d, Score: %.5f ' % (i,v))\n","plt.bar([x for x in range(len(importance))], importance, color='green')\n","x_label = ['H_1','Me_1','OMe_1','CF3_1','Et_1','OEt_1','Pr_1','iPr_1','tBu_1','Ph_1','H_2','Me_2','Et_2','Pr_2','iPr_2','tBu_2']\n","plt.xticks(range(len(importance)),x_label,weight='bold', rotation = 90)\n","plt.yticks(weight='bold')\n","plt.rcParams['figure.dpi']=1200\n","plt.show()"],"metadata":{"id":"WaNI9aZ_fRn0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Parity plot with XGBoost model**"],"metadata":{"id":"9_O19CfgsQEP"}},{"cell_type":"code","source":["reg = xgb.XGBRegressor(n_estimators=100, min_child_weight=3, learning_rate=0.05, booster='gbtree', max_depth=5, base_score=0.5)\n","reg.fit(x_train,y_train)\n","pred_train = reg.predict(x_train)\n","pred_test = reg.predict(x_test)\n","plt.scatter(y_test, pred_test,color='r',s=70, label = 'Test set')\n","sns.regplot(x=y_train,y=pred_train,color='g',scatter_kws={'s':70},line_kws={'color':'blue'}, label='Train set')\n","plt.xlabel('Predicted Energy (eV)', fontsize = 12,weight='bold')\n","plt.ylabel('DFT Calculated Energy (eV)', fontsize = 12,weight='bold')\n","plt.xlim(0.15,1.4)\n","plt.ylim(0.15,1.4)\n","plt.gca().set_aspect('equal', adjustable='box')\n","plt.xticks(weight='bold')\n","plt.yticks(weight='bold')\n","plt.legend(fontsize=16)\n","plt.rc('axes', edgecolor='black')\n","plt.rcParams['figure.dpi']=1200\n","plt.show()"],"metadata":{"id":"xEnZq8KUsn92"},"execution_count":null,"outputs":[]}]}